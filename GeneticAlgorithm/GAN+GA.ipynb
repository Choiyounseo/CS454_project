{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN+GA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BZrb3knlLqn",
        "colab_type": "code",
        "outputId": "512fc3e0-cf07-4c2c-acfc-7e9c73b3cd1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pip install deap"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deap in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deap) (1.17.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6KCDNJNmCqe",
        "colab_type": "code",
        "outputId": "ca6b6745-7c2f-44df-d0a5-50e2ce9fda4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "gdrive_root = '/gdrive/My Drive'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwXRTu0KldZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu, nc, nz, ngf):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(nz, ngf * 8, 7, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 7 x 7\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 7 x 7\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 14 x 14\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 14 x 14\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 28 x 28\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r02KmvHGmKJY",
        "colab_type": "code",
        "outputId": "f756ee4f-35d4-4011-8cc3-8729518b7d52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.utils as utils\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def imshow(img):\n",
        "    img = (img + 1) / 2\n",
        "    img = img.squeeze()\n",
        "    np_img = img.numpy()\n",
        "    plt.imshow(np_img, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "def imshow_grid(img):\n",
        "    img = utils.make_grid(img.cpu().detach())\n",
        "    img = (img + 1) / 2\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Hyper parameters\n",
        "params = {\n",
        "    'input_size': 28,  # image size 1x64x64\n",
        "    'batch_size': 64,  # batch size\n",
        "    'pop_size': 100,   # population size\n",
        "    'nc': 1,  # number of channels\n",
        "    'nz': 100,  # size of z latent vector\n",
        "    'ngf': 64,  # size of feature maps in generator\n",
        "    'ndf': 32,  # size of feature maps in discriminator\n",
        "    'num_epochs': 1000,  # number of epochs\n",
        "    'lr': 0.0001,  # learning rate\n",
        "    'beta1': 0.5,   # beta1 for adam optimizer\n",
        "    'ngpu': 1,  # number of GPU\n",
        "    'lambda_gp': 10,  # loss weight for gradient penalty\n",
        "    'n_critic': 5,\n",
        "}\n",
        "\n",
        "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Generator(ngpu, nc, nz, ngf)\n",
        "# 이 부분에서 체크포인트 위치 설정.\n",
        "netG = Generator(params['ngpu'], params['nc'], params['nz'], params['ngf'])\n",
        "netG.load_state_dict(torch.load(gdrive_root + '/checkpoints/netG_12500.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "# transform\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
        "# data sets and data loader\n",
        "train_data = datasets.MNIST(root='data/', train=True, transform=transform, download=True)\n",
        "train_data_loader = DataLoader(train_data, params['batch_size'], shuffle=False)\n",
        "first_batch = train_data_loader.__iter__().__next__()  # first batch of MNIST data set : torch.Size([64x, 1, 28, 28])\n",
        "print(first_batch[0][0].shape)  # torch.Size([1, 28, 28])\n",
        "#imshow(first_batch[0][0])  # plot the image of first batch"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxREQRNHrbZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input image for defense GAN\n",
        "# For the test purpose, we will use MNIST data sample first.\n",
        "# fgsm_image : torch.Size([1, 28, 28]). This is image x.\n",
        "fgsm_image = first_batch[0][0]  # torch.Size([1, 28, 28]). This should be fgsm_image later on.\n",
        "\n",
        "# evalFunc 에서 numpy 형태로 계산하려고 모양 수정 / 일단은 그냥 28, 28로 했는데 나중엔 일렬로 계산해도 될 듯.\n",
        "x = fgsm_image.view(28,28).numpy()\n",
        "#imshow(fgsm_image)\n",
        "\n",
        "# individual은 numpy array\n",
        "# numpy array 가 들어오면 -> tensor로 바꾸고, netG input 모양에 맞춰줌.\n",
        "# netG의 output이 tensor 형태이므로, numpy로 바꿔서 계산.. (이부분 텐서 형태에서 계산으로 추후 수정하면 될 듯.)\n",
        "def evalFunc(individual):\n",
        "    individual = torch.from_numpy(individual).view(1, 100, 1, 1)\n",
        "    return np.linalg.norm(netG(individual).view(28, 28).detach().numpy() - x, ord=2)**2,\n",
        "\n",
        "# Initial population for GA\n",
        "# initial_population : torch.Size([100, 100, 1, 1]), This has 100 latent vectors z (z is torch.Size([100, 1, 1])).\n",
        "# for example, initial_population[0] is z_0, initial_population[1] is z_1, ..., initial_population[99] is z_99.\n",
        "initial_population = torch.FloatTensor(params['pop_size'], params['nz'], 1, 1).normal_(0, 1)\n",
        "#print(initial_population.shape)  # torch.Size([100, 100, 1, 1])\n",
        "\n",
        "\n",
        "# 이 부분은 evalFunc 이 잘 작동하는지 확인하는 부분입니당.\n",
        "# z 가 길이 100짜리 1차원 numpy array 라고 가정하고(GA 에서 기본 individual 형태) evalFunc에 넣은뒤, 출력\n",
        "# z = torch.FloatTensor(1, params['nz'], 1, 1).normal_(0, 1)\n",
        "# z = z.view(100).numpy()\n",
        "# print(evalFunc(z))\n",
        "\n",
        "# initial_population를 numpy로 타입 맞춰주기.\n",
        "initial_population = initial_population.view(100, 100).numpy()\n",
        "\n",
        "def initIndividual(icls, content):\n",
        "  return icls(content)\n",
        "\n",
        "def initPopulation(pcls, ind_init):\n",
        "  return pcls(ind_init(c) for c in initial_population)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "navWYmQBsJUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from deap import creator, base, tools, algorithms\n",
        "\n",
        "# TOCHECK:  will we use algorithm modules?\n",
        "\n",
        "'''\n",
        "Fitness : single optimization, minimize ||G(z) - x||^2_2\n",
        "'''\n",
        "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
        "creator.create(\"Individual\", np.ndarray, fitness=creator.FitnessMin) # minimizing the fitness value\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "\n",
        "'''\n",
        "[HYPERPARAMETERS]\n",
        "List of Floats: individuals composed of 'IND_SIZE' floating point numbers\n",
        "IND_SIZE: z의 dimension으로 보면 될듯\n",
        "POPULATION\n",
        "CXPB: probability of crossover\n",
        "MUTPB: probability of mutation\n",
        "'''\n",
        "IND_SIZE = 100\n",
        "POPULATION = 100\n",
        "CXPB, MUTPB = 0.2, 0.2\n",
        "GENERATIONS = 100\n",
        "\n",
        "toolbox.register(\"attr_float\", random.random)\n",
        "# toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=IND_SIZE)\n",
        "toolbox.register(\"individual\", initIndividual, creator.Individual)\n",
        "toolbox.register(\"population\", initPopulation, list, toolbox.individual)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO-h_eSNry73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cxTwoPointCopy(ind1, ind2):\n",
        "    \"\"\"Execute a two points crossover with copy on the input individuals. The\n",
        "    copy is required because the slicing in numpy returns a view of the data,\n",
        "    which leads to a self overwritting in the swap operation. It prevents\n",
        "    ::\n",
        "    \n",
        "        >>> import numpy\n",
        "        >>> a = numpy.array((1,2,3,4))\n",
        "        >>> b = numpy.array((5,6,7,8))\n",
        "        >>> a[1:3], b[1:3] = b[1:3], a[1:3]\n",
        "        >>> print(a)\n",
        "        [1 6 7 4]\n",
        "        >>> print(b)\n",
        "        [5 6 7 8]\n",
        "    \"\"\"\n",
        "    size = len(ind1)\n",
        "    cxpoint1 = random.randint(1, size)\n",
        "    cxpoint2 = random.randint(1, size - 1)\n",
        "    if cxpoint2 >= cxpoint1:\n",
        "        cxpoint2 += 1\n",
        "    else: # Swap the two cx points\n",
        "        cxpoint1, cxpoint2 = cxpoint2, cxpoint1\n",
        "\n",
        "    ind1[cxpoint1:cxpoint2], ind2[cxpoint1:cxpoint2] \\\n",
        "        = ind2[cxpoint1:cxpoint2].copy(), ind1[cxpoint1:cxpoint2].copy()\n",
        "        \n",
        "    return ind1, ind2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_HRnfCXruGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Evaluation Function: ||G(z) - x||^2_2 <- use 'np.linalg.norm( , ord=2)**2'\n",
        "'''\n",
        "# TODO : need to change mate, mutate, select operator\n",
        "# can check in here : https://deap.readthedocs.io/en/master/api/tools.html\n",
        "toolbox.register(\"evaluate\", evalFunc)\n",
        "toolbox.register(\"mate\", cxTwoPointCopy)\n",
        "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO_oSiTyr6gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    random.seed(777)\n",
        "\n",
        "    # pop = toolbox.population(n=POPULATION)\n",
        "    pop = toolbox.population()\n",
        "    \n",
        "    print(\"Start of evolution\")\n",
        "\n",
        "    # Evaluate the entire population\n",
        "    # print(fitnesses) -> [(84,), (105,), (96,), (104,), (94,),  ... ] 이런식으로 저장됨.\n",
        "    fitnesses = list(map(toolbox.evaluate, pop))\n",
        "    for ind, fit in zip(pop, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    # Extracting all the fitnesses of \n",
        "    fits = [ind.fitness.values[0] for ind in pop]\n",
        "\n",
        "    # Variable keeping track of the number of generations\n",
        "    g = 0\n",
        "    \n",
        "    # Begin the evolution\n",
        "    while min(fits) > 10 and g < GENERATIONS:\n",
        "        # A new generation\n",
        "        g = g + 1\n",
        "        print(\"-- Generation %i --\" % g)\n",
        "        \n",
        "        # Select the next generation individuals\n",
        "        # len(pop) -> 50, len(pop[0]) -> 5\n",
        "        offspring = toolbox.select(pop, len(pop))\n",
        "\n",
        "        # Clone the selected individuals\n",
        "        offspring = list(map(toolbox.clone, offspring))\n",
        "    \n",
        "        # Apply crossover and mutation on the offspring\n",
        "        '''\n",
        "        they modify those individuals within the toolbox container \n",
        "        and we do not need to reassign their results.\n",
        "        '''\n",
        "        # TODO: want p_new1 = p_m - beta(p_m - p_d), p_new2 = p_m + beta(p_m - p_d)\n",
        "        # want to customize mutation method... there is no proper mutation operator in deap.tools...\n",
        "\n",
        "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
        "            if random.random() < CXPB:\n",
        "                toolbox.mate(child1, child2)\n",
        "                del child1.fitness.values\n",
        "                del child2.fitness.values\n",
        "\n",
        "        for mutant in offspring:\n",
        "            if random.random() < MUTPB:\n",
        "                toolbox.mutate(mutant)\n",
        "                del mutant.fitness.values\n",
        "    \n",
        "        # Evaluate the individuals with an invalid fitness\n",
        "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
        "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
        "        for ind, fit in zip(invalid_ind, fitnesses):\n",
        "            ind.fitness.values = fit\n",
        "        \n",
        "        # The population is entirely replaced by the offspring\n",
        "        pop[:] = offspring\n",
        "\n",
        "        # Gather all the fitnesses in one list and print the stats\n",
        "        fits = [ind.fitness.values[0] for ind in pop]\n",
        "        \n",
        "        length = len(pop)\n",
        "        mean = sum(fits) / length\n",
        "        sum2 = sum(x*x for x in fits)\n",
        "        std = abs(sum2 / length - mean**2)**0.5\n",
        "        \n",
        "        print(\"  Min %s\" % min(fits))\n",
        "        print(\"  Max %s\" % max(fits))\n",
        "        print(\"  Avg %s\" % mean)\n",
        "        print(\"  Std %s\" % std)\n",
        "\n",
        "\n",
        "    print(\"-- End of (successful) evolution --\")\n",
        "    \n",
        "    best_ind = tools.selBest(pop, 1)[0]\n",
        "    print(\"Best individual is %s, %s\" % (best_ind, best_ind.fitness.values))\n",
        "    print(best_ind.fitness.values)\n",
        "    return best_ind\n",
        "# main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kb7nChgmRDK",
        "colab_type": "code",
        "outputId": "13c5b47b-3287-43c0-e6e5-4406b54085cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# For each generation, select the latent vector z* that minimizes fitness, and do the following.\n",
        "# z = torch.FloatTensor(1, params['nz'], 1, 1).normal_(0, 1)  # torch.Size([100, 1, 1]). This should be z* later on.\n",
        "z = main()\n",
        "print(\"the shape of latent vector : \" + str(z.shape))\n",
        "gen_image = netG(z)  # torch.Size([1, 28, 28]). This is the generated image that we want to see for each generation.\n",
        "# Because gen_image should step closer to fgsm_image x for each generation.\n",
        "print(\"the shape of generated image : \" + str(gen_image.shape))\n",
        "imshow(gen_image.detach())  # plot the image of generated image\n",
        "\n",
        "# After GA, give generated image as input to each classifier (use gen_image)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of evolution\n",
            "-- Generation 1 --\n",
            "  Min 90.27580445655622\n",
            "  Max 229.24889287153292\n",
            "  Avg 136.12714514527966\n",
            "  Std 25.63002983509303\n",
            "-- Generation 2 --\n",
            "  Min 88.8468983091625\n",
            "  Max 180.36782839781154\n",
            "  Avg 120.27503691832396\n",
            "  Std 18.52540955627759\n",
            "-- Generation 3 --\n",
            "  Min 87.01007298898185\n",
            "  Max 183.62943620287206\n",
            "  Avg 112.36509436032807\n",
            "  Std 21.176799003961396\n",
            "-- Generation 4 --\n",
            "  Min 79.44566800470147\n",
            "  Max 173.00740171893358\n",
            "  Avg 102.64009262062704\n",
            "  Std 14.195602749103193\n",
            "-- Generation 5 --\n",
            "  Min 78.04516051984046\n",
            "  Max 144.82873410872708\n",
            "  Avg 98.78725587882346\n",
            "  Std 12.73889363476736\n",
            "-- Generation 6 --\n",
            "  Min 71.09774215263678\n",
            "  Max 156.35504103195217\n",
            "  Avg 93.08443443310563\n",
            "  Std 13.427954403786352\n",
            "-- Generation 7 --\n",
            "  Min 46.51384625376545\n",
            "  Max 137.26806573900922\n",
            "  Avg 88.57043568557413\n",
            "  Std 15.205257811130448\n",
            "-- Generation 8 --\n",
            "  Min 46.51384625376545\n",
            "  Max 169.93001743243076\n",
            "  Avg 83.41985638968919\n",
            "  Std 18.130310107521346\n",
            "-- Generation 9 --\n",
            "  Min 46.51384625376545\n",
            "  Max 150.0591120911131\n",
            "  Avg 74.96444445138329\n",
            "  Std 15.634039323029535\n",
            "-- Generation 10 --\n",
            "  Min 45.6751063972124\n",
            "  Max 121.18449181059623\n",
            "  Avg 65.30574055026926\n",
            "  Std 14.06836222361036\n",
            "-- Generation 11 --\n",
            "  Min 39.679266568200774\n",
            "  Max 92.2415255899623\n",
            "  Avg 55.90017481604128\n",
            "  Std 9.437420018194967\n",
            "-- Generation 12 --\n",
            "  Min 39.679266568200774\n",
            "  Max 84.1961691497354\n",
            "  Avg 50.650776098685775\n",
            "  Std 7.442280719887314\n",
            "-- Generation 13 --\n",
            "  Min 37.037236502561655\n",
            "  Max 88.86716112949307\n",
            "  Avg 48.71566695674282\n",
            "  Std 7.4197888167748385\n",
            "-- Generation 14 --\n",
            "  Min 37.037236502561655\n",
            "  Max 133.05347133573468\n",
            "  Avg 49.00523685201038\n",
            "  Std 12.669208784118856\n",
            "-- Generation 15 --\n",
            "  Min 37.037236502561655\n",
            "  Max 64.9731538912747\n",
            "  Avg 44.33339468194663\n",
            "  Std 5.228225701735236\n",
            "-- Generation 16 --\n",
            "  Min 36.1156952984004\n",
            "  Max 102.45692521426827\n",
            "  Avg 42.996107542973206\n",
            "  Std 7.411105837858545\n",
            "-- Generation 17 --\n",
            "  Min 35.83573620871539\n",
            "  Max 76.68222574331685\n",
            "  Avg 40.561103921797496\n",
            "  Std 5.582831649870712\n",
            "-- Generation 18 --\n",
            "  Min 35.57593086806287\n",
            "  Max 80.60120769894274\n",
            "  Avg 40.23802394166922\n",
            "  Std 7.067799620673347\n",
            "-- Generation 19 --\n",
            "  Min 35.57593086806287\n",
            "  Max 71.90397117640623\n",
            "  Avg 38.81056499401913\n",
            "  Std 4.857623941525422\n",
            "-- Generation 20 --\n",
            "  Min 35.40959103802743\n",
            "  Max 56.930861402867095\n",
            "  Avg 38.025411837381654\n",
            "  Std 3.556328692395589\n",
            "-- Generation 21 --\n",
            "  Min 34.9536236400163\n",
            "  Max 62.82115284563042\n",
            "  Avg 38.060353105367305\n",
            "  Std 4.651855612176925\n",
            "-- Generation 22 --\n",
            "  Min 34.9536236400163\n",
            "  Max 50.302362016708685\n",
            "  Avg 37.08253654798494\n",
            "  Std 2.5945207886311534\n",
            "-- Generation 23 --\n",
            "  Min 33.488808148306134\n",
            "  Max 57.810252090432414\n",
            "  Avg 37.18348728329768\n",
            "  Std 3.572725124136622\n",
            "-- Generation 24 --\n",
            "  Min 33.488808148306134\n",
            "  Max 48.72811562527204\n",
            "  Avg 36.25927597532828\n",
            "  Std 2.3951636511640304\n",
            "-- Generation 25 --\n",
            "  Min 30.04826579574933\n",
            "  Max 61.47776028156022\n",
            "  Avg 36.31398836413733\n",
            "  Std 4.24724237930805\n",
            "-- Generation 26 --\n",
            "  Min 29.03989675942853\n",
            "  Max 48.07966187245461\n",
            "  Avg 35.566262111244384\n",
            "  Std 3.029383942841392\n",
            "-- Generation 27 --\n",
            "  Min 29.03989675942853\n",
            "  Max 55.19096056022681\n",
            "  Avg 34.64113136198602\n",
            "  Std 3.0656067276600947\n",
            "-- Generation 28 --\n",
            "  Min 29.03989675942853\n",
            "  Max 65.96378152567559\n",
            "  Avg 34.82580641051165\n",
            "  Std 4.413161171532737\n",
            "-- Generation 29 --\n",
            "  Min 29.03989675942853\n",
            "  Max 52.9712464995157\n",
            "  Avg 34.640750143432335\n",
            "  Std 4.093883990830627\n",
            "-- Generation 30 --\n",
            "  Min 31.20860053200522\n",
            "  Max 52.468316344897175\n",
            "  Avg 34.30363684333899\n",
            "  Std 3.7297304271440916\n",
            "-- Generation 31 --\n",
            "  Min 30.356955011184482\n",
            "  Max 47.3126075137443\n",
            "  Avg 34.12247885066921\n",
            "  Std 3.8759109558070346\n",
            "-- Generation 32 --\n",
            "  Min 30.356955011184482\n",
            "  Max 52.046959230216316\n",
            "  Avg 33.98515751804081\n",
            "  Std 4.018654644687836\n",
            "-- Generation 33 --\n",
            "  Min 30.356955011184482\n",
            "  Max 55.40540892805461\n",
            "  Avg 33.442090148942896\n",
            "  Std 4.139135345342317\n",
            "-- Generation 34 --\n",
            "  Min 30.356955011184482\n",
            "  Max 51.580461922458426\n",
            "  Avg 33.54725344394856\n",
            "  Std 4.475290392320809\n",
            "-- Generation 35 --\n",
            "  Min 30.356955011184482\n",
            "  Max 54.4339801604749\n",
            "  Avg 33.21902808921718\n",
            "  Std 4.676816467593075\n",
            "-- Generation 36 --\n",
            "  Min 30.356955011184482\n",
            "  Max 45.12910139681503\n",
            "  Avg 31.893047676804134\n",
            "  Std 2.9000579918122806\n",
            "-- Generation 37 --\n",
            "  Min 30.356955011184482\n",
            "  Max 57.10905859907598\n",
            "  Avg 31.78466543722755\n",
            "  Std 3.2987504172505666\n",
            "-- Generation 38 --\n",
            "  Min 29.736525461096107\n",
            "  Max 56.37734459841249\n",
            "  Avg 32.01767298248632\n",
            "  Std 3.9034903030547086\n",
            "-- Generation 39 --\n",
            "  Min 29.736525461096107\n",
            "  Max 49.1803299452622\n",
            "  Avg 31.67215525904692\n",
            "  Std 3.3163204277682903\n",
            "-- Generation 40 --\n",
            "  Min 29.733374040890567\n",
            "  Max 48.4443551630759\n",
            "  Avg 31.963683347457508\n",
            "  Std 3.8747114113009156\n",
            "-- Generation 41 --\n",
            "  Min 29.736525461096107\n",
            "  Max 44.147330583020675\n",
            "  Avg 31.16195618529642\n",
            "  Std 2.3829983486404718\n",
            "-- Generation 42 --\n",
            "  Min 29.606188020463605\n",
            "  Max 55.60393711466736\n",
            "  Avg 31.472190647325068\n",
            "  Std 3.867888683632135\n",
            "-- Generation 43 --\n",
            "  Min 29.606188020463605\n",
            "  Max 46.03776943491198\n",
            "  Avg 30.958445941068412\n",
            "  Std 3.096611086760933\n",
            "-- Generation 44 --\n",
            "  Min 29.28530740321753\n",
            "  Max 69.36727655441155\n",
            "  Avg 32.29784964487158\n",
            "  Std 6.35634613549529\n",
            "-- Generation 45 --\n",
            "  Min 28.58980859157691\n",
            "  Max 47.523758820300145\n",
            "  Avg 31.066016497862876\n",
            "  Std 3.2801237810367065\n",
            "-- Generation 46 --\n",
            "  Min 28.554440988285933\n",
            "  Max 70.08819201353708\n",
            "  Avg 31.524910332754597\n",
            "  Std 5.143963536366392\n",
            "-- Generation 47 --\n",
            "  Min 28.21405416718426\n",
            "  Max 48.54833834144756\n",
            "  Avg 31.003404043407954\n",
            "  Std 3.987313024262434\n",
            "-- Generation 48 --\n",
            "  Min 27.533223062983097\n",
            "  Max 43.195151799740415\n",
            "  Avg 30.441129690831257\n",
            "  Std 2.6490816241913935\n",
            "-- Generation 49 --\n",
            "  Min 27.533223062983097\n",
            "  Max 51.544160464141896\n",
            "  Avg 30.871295983835935\n",
            "  Std 3.847816736956142\n",
            "-- Generation 50 --\n",
            "  Min 27.533223062983097\n",
            "  Max 54.185971610341085\n",
            "  Avg 31.527918273242413\n",
            "  Std 5.524775712405282\n",
            "-- Generation 51 --\n",
            "  Min 27.29319991536636\n",
            "  Max 58.3281912745972\n",
            "  Avg 31.096712434321088\n",
            "  Std 5.803429718779073\n",
            "-- Generation 52 --\n",
            "  Min 25.217125128259795\n",
            "  Max 59.91497812327361\n",
            "  Avg 30.17264089313133\n",
            "  Std 5.46899476676002\n",
            "-- Generation 53 --\n",
            "  Min 25.217125128259795\n",
            "  Max 66.09113347767652\n",
            "  Avg 29.57455348648871\n",
            "  Std 5.41827274713839\n",
            "-- Generation 54 --\n",
            "  Min 25.01670162579194\n",
            "  Max 61.425279112264434\n",
            "  Avg 28.79536026555239\n",
            "  Std 5.331432470163107\n",
            "-- Generation 55 --\n",
            "  Min 25.01670162579194\n",
            "  Max 64.46095580956262\n",
            "  Avg 29.66561857958789\n",
            "  Std 6.525504453416714\n",
            "-- Generation 56 --\n",
            "  Min 25.01670162579194\n",
            "  Max 59.95358426258122\n",
            "  Avg 28.430811741931485\n",
            "  Std 5.941095513054068\n",
            "-- Generation 57 --\n",
            "  Min 25.01670162579194\n",
            "  Max 69.21414321498014\n",
            "  Avg 28.433640299797897\n",
            "  Std 6.524347525520392\n",
            "-- Generation 58 --\n",
            "  Min 25.01670162579194\n",
            "  Max 47.991944377867185\n",
            "  Avg 27.833285572788608\n",
            "  Std 5.021592203672762\n",
            "-- Generation 59 --\n",
            "  Min 25.01670162579194\n",
            "  Max 52.83922137998252\n",
            "  Avg 27.44956404777966\n",
            "  Std 5.144005204039448\n",
            "-- Generation 60 --\n",
            "  Min 24.613006271744098\n",
            "  Max 70.64547936386953\n",
            "  Avg 27.257501432032395\n",
            "  Std 6.332732304103363\n",
            "-- Generation 61 --\n",
            "  Min 24.613006271744098\n",
            "  Max 87.76825160649969\n",
            "  Avg 28.126118775446002\n",
            "  Std 7.805429733743528\n",
            "-- Generation 62 --\n",
            "  Min 24.613006271744098\n",
            "  Max 59.48315787659544\n",
            "  Avg 28.097099934548833\n",
            "  Std 6.648743315747546\n",
            "-- Generation 63 --\n",
            "  Min 24.613006271744098\n",
            "  Max 67.43272892505865\n",
            "  Avg 28.393483801657986\n",
            "  Std 7.29324202150422\n",
            "-- Generation 64 --\n",
            "  Min 24.613006271744098\n",
            "  Max 53.98594320017787\n",
            "  Avg 26.68823471010418\n",
            "  Std 4.49865019401563\n",
            "-- Generation 65 --\n",
            "  Min 24.51662916678856\n",
            "  Max 49.34476439710215\n",
            "  Avg 27.004225602807736\n",
            "  Std 4.656835486678902\n",
            "-- Generation 66 --\n",
            "  Min 24.51662916678856\n",
            "  Max 53.704588907138714\n",
            "  Avg 26.80650822424035\n",
            "  Std 5.227139477971332\n",
            "-- Generation 67 --\n",
            "  Min 24.51662916678856\n",
            "  Max 64.27915811917865\n",
            "  Avg 27.203425560853884\n",
            "  Std 7.1440299579793285\n",
            "-- Generation 68 --\n",
            "  Min 24.51662916678856\n",
            "  Max 39.24437424568259\n",
            "  Avg 25.90264517972746\n",
            "  Std 3.0096727724888535\n",
            "-- Generation 69 --\n",
            "  Min 24.51662916678856\n",
            "  Max 53.67310874768941\n",
            "  Avg 26.498022326289473\n",
            "  Std 4.93589925441019\n",
            "-- Generation 70 --\n",
            "  Min 24.51662916678856\n",
            "  Max 57.65109188261795\n",
            "  Avg 27.369542582781424\n",
            "  Std 5.847285900079139\n",
            "-- Generation 71 --\n",
            "  Min 24.51662916678856\n",
            "  Max 89.96637632776492\n",
            "  Avg 27.9312930319898\n",
            "  Std 10.29203199043327\n",
            "-- Generation 72 --\n",
            "  Min 24.51662916678856\n",
            "  Max 67.10451107058907\n",
            "  Avg 27.696264123931705\n",
            "  Std 7.990820960776703\n",
            "-- Generation 73 --\n",
            "  Min 24.51662916678856\n",
            "  Max 61.629101634395056\n",
            "  Avg 26.595138723726258\n",
            "  Std 5.545927880128388\n",
            "-- Generation 74 --\n",
            "  Min 24.51662916678856\n",
            "  Max 58.61233755524245\n",
            "  Avg 26.87898349470196\n",
            "  Std 6.529200683326796\n",
            "-- Generation 75 --\n",
            "  Min 24.51662916678856\n",
            "  Max 65.47168232106196\n",
            "  Avg 27.87710797988365\n",
            "  Std 8.523924814004491\n",
            "-- Generation 76 --\n",
            "  Min 24.51662916678856\n",
            "  Max 58.15276948444898\n",
            "  Avg 28.129161010156324\n",
            "  Std 8.225042594595942\n",
            "-- Generation 77 --\n",
            "  Min 24.51662916678856\n",
            "  Max 54.243172461903896\n",
            "  Avg 26.95344498061957\n",
            "  Std 6.098891596253648\n",
            "-- Generation 78 --\n",
            "  Min 24.51662916678856\n",
            "  Max 67.77999308578728\n",
            "  Avg 28.253640172030053\n",
            "  Std 7.794508785669133\n",
            "-- Generation 79 --\n",
            "  Min 24.51662916678856\n",
            "  Max 65.20994990844338\n",
            "  Avg 27.46139102516363\n",
            "  Std 7.675253494423055\n",
            "-- Generation 80 --\n",
            "  Min 24.51662916678856\n",
            "  Max 55.862806878978745\n",
            "  Avg 27.5390031069216\n",
            "  Std 6.901170457300431\n",
            "-- Generation 81 --\n",
            "  Min 24.51662916678856\n",
            "  Max 52.075233596450744\n",
            "  Avg 27.062813237480533\n",
            "  Std 5.746970352725001\n",
            "-- Generation 82 --\n",
            "  Min 24.128359427996656\n",
            "  Max 62.07261786364165\n",
            "  Avg 27.050792870021816\n",
            "  Std 7.257831638107165\n",
            "-- Generation 83 --\n",
            "  Min 24.128359427996656\n",
            "  Max 53.95499712457331\n",
            "  Avg 26.793988223606686\n",
            "  Std 5.5573721116956305\n",
            "-- Generation 84 --\n",
            "  Min 23.91012608825349\n",
            "  Max 65.47168232106196\n",
            "  Avg 26.781526620911492\n",
            "  Std 6.262307190936572\n",
            "-- Generation 85 --\n",
            "  Min 23.91012608825349\n",
            "  Max 69.83690453498639\n",
            "  Avg 28.118865949693927\n",
            "  Std 9.262765425443593\n",
            "-- Generation 86 --\n",
            "  Min 23.91012608825349\n",
            "  Max 60.665602299417515\n",
            "  Avg 26.66643669819442\n",
            "  Std 7.357305370225216\n",
            "-- Generation 87 --\n",
            "  Min 23.91012608825349\n",
            "  Max 64.85878827225315\n",
            "  Avg 26.705527909131725\n",
            "  Std 6.956420200321882\n",
            "-- Generation 88 --\n",
            "  Min 23.91012608825349\n",
            "  Max 65.52399598386637\n",
            "  Avg 26.009286648666176\n",
            "  Std 6.593798404496689\n",
            "-- Generation 89 --\n",
            "  Min 23.016396429189626\n",
            "  Max 64.94611320542845\n",
            "  Avg 26.65260150534211\n",
            "  Std 7.53532713038053\n",
            "-- Generation 90 --\n",
            "  Min 23.016396429189626\n",
            "  Max 68.55869564240857\n",
            "  Avg 27.511204569562825\n",
            "  Std 8.475156000137169\n",
            "-- Generation 91 --\n",
            "  Min 23.016396429189626\n",
            "  Max 90.2399075468793\n",
            "  Avg 27.84164999815187\n",
            "  Std 11.170527400524634\n",
            "-- Generation 92 --\n",
            "  Min 23.016396429189626\n",
            "  Max 62.005381530358136\n",
            "  Avg 27.070271835936573\n",
            "  Std 8.185622300300064\n",
            "-- Generation 93 --\n",
            "  Min 23.016396429189626\n",
            "  Max 71.28724162245726\n",
            "  Avg 26.287662963122756\n",
            "  Std 7.344219121342853\n",
            "-- Generation 94 --\n",
            "  Min 23.016396429189626\n",
            "  Max 61.14362524765602\n",
            "  Avg 26.859410202674816\n",
            "  Std 7.52656883400781\n",
            "-- Generation 95 --\n",
            "  Min 22.58378673761854\n",
            "  Max 85.34927651019188\n",
            "  Avg 27.498698839463085\n",
            "  Std 10.622952504304234\n",
            "-- Generation 96 --\n",
            "  Min 22.58378673761854\n",
            "  Max 56.134216823269526\n",
            "  Avg 26.207149382342724\n",
            "  Std 6.394381134449619\n",
            "-- Generation 97 --\n",
            "  Min 22.58378673761854\n",
            "  Max 58.06466713516443\n",
            "  Avg 24.860221893476055\n",
            "  Std 5.567410109955009\n",
            "-- Generation 98 --\n",
            "  Min 22.58378673761854\n",
            "  Max 62.447029206902016\n",
            "  Avg 24.154537930210218\n",
            "  Std 4.458625527316123\n",
            "-- Generation 99 --\n",
            "  Min 21.58202764255202\n",
            "  Max 70.15974723475756\n",
            "  Avg 26.21549630777119\n",
            "  Std 7.801703898798979\n",
            "-- Generation 100 --\n",
            "  Min 21.58202764255202\n",
            "  Max 73.55466630004321\n",
            "  Avg 25.567685757433583\n",
            "  Std 7.556323559181065\n",
            "-- End of (successful) evolution --\n",
            "Best individual is [ 0.         -0.5961266   1.7001076  -1.1472052   0.          1.\n",
            "  0.          1.         -0.9735274   0.          0.38723692 -1.0004238\n",
            " -0.5272783   0.46343732  0.         -0.52884966  0.          0.\n",
            " -0.19304433  0.49465647  0.          0.36483625 -1.3416544   0.\n",
            "  1.8972198   0.         -0.36262667 -1.1637266  -0.62898     0.6306828\n",
            "  0.         -2.0116892  -1.7956578   0.          0.         -0.72795045\n",
            "  0.          1.          1.0262148   1.         -2.7995045   0.\n",
            "  1.         -0.6346043   1.          0.          1.1868888   1.\n",
            " -0.2988867   0.          0.3980379  -1.702004    1.          0.\n",
            "  1.         -1.0300592  -1.4437912   0.          1.          0.\n",
            "  0.20457675 -0.22392888  0.          0.41191077 -1.0961543   0.\n",
            "  0.          0.5430754  -0.35083616  0.6898592   0.05689857 -0.10938265\n",
            "  0.         -0.4539394   0.         -0.8980222   1.5942465   0.\n",
            "  0.          0.          0.20199849  0.         -0.01899352  0.7173169\n",
            "  1.1904719  -0.47265673 -1.7249361   1.          0.9765296   0.34570685\n",
            " -1.0169494  -1.8621811   0.          1.8082167   1.0733701   0.53079736\n",
            "  0.27428252  1.3300097   1.          0.        ], (21.58202764255202,)\n",
            "(21.58202764255202,)\n",
            "the shape of latent vector : (100,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b8df5db53d66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the shape of latent vector : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgen_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# torch.Size([1, 28, 28]). This is the generated image that we want to see for each generation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Because gen_image should step closer to fgsm_image x for each generation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the shape of generated image : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-3c976ee0c0ec>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    776\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    777\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: conv_transpose2d(): argument 'input' (position 1) must be Tensor, not Individual"
          ]
        }
      ]
    }
  ]
}