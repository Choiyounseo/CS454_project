{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN+GA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BZrb3knlLqn",
        "colab_type": "code",
        "outputId": "9fe093e5-61b6-4fba-824f-90fee9ab4faa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "pip install deap"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deap\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/98/3166fb5cfa47bf516e73575a1515734fe3ce05292160db403ae542626b32/deap-1.3.0-cp36-cp36m-manylinux2010_x86_64.whl (151kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 13.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 40kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 61kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 71kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deap) (1.17.4)\n",
            "Installing collected packages: deap\n",
            "Successfully installed deap-1.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6KCDNJNmCqe",
        "colab_type": "code",
        "outputId": "d6e42f33-fc2f-4e3c-887a-72304c399c64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "gdrive_root = '/gdrive/My Drive'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwXRTu0KldZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu, nc, nz, ngf):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(nz, ngf * 8, 7, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 7 x 7\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 7 x 7\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 14 x 14\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 14 x 14\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 28 x 28\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r02KmvHGmKJY",
        "colab_type": "code",
        "outputId": "44fadec7-6b66-4a10-dec0-593e1bde22f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.utils as utils\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def imshow(img):\n",
        "    img = (img + 1) / 2\n",
        "    img = img.squeeze()\n",
        "    np_img = img.numpy()\n",
        "    plt.imshow(np_img, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "def imshow_grid(img):\n",
        "    img = utils.make_grid(img.cpu().detach())\n",
        "    img = (img + 1) / 2\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Hyper parameters\n",
        "params = {\n",
        "    'input_size': 28,  # image size 1x64x64\n",
        "    'batch_size': 64,  # batch size\n",
        "    'pop_size': 100,   # population size\n",
        "    'nc': 1,  # number of channels\n",
        "    'nz': 100,  # size of z latent vector\n",
        "    'ngf': 64,  # size of feature maps in generator\n",
        "    'ndf': 32,  # size of feature maps in discriminator\n",
        "    'num_epochs': 1000,  # number of epochs\n",
        "    'lr': 0.0001,  # learning rate\n",
        "    'beta1': 0.5,   # beta1 for adam optimizer\n",
        "    'ngpu': 1,  # number of GPU\n",
        "    'lambda_gp': 10,  # loss weight for gradient penalty\n",
        "    'n_critic': 5,\n",
        "}\n",
        "\n",
        "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Generator(ngpu, nc, nz, ngf)\n",
        "# 이 부분에서 체크포인트 위치 설정.\n",
        "netG = Generator(params['ngpu'], params['nc'], params['nz'], params['ngf'])\n",
        "netG.load_state_dict(torch.load(gdrive_root + '/checkpoints/netG_12500.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "# transform\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
        "# data sets and data loader\n",
        "train_data = datasets.MNIST(root='data/', train=True, transform=transform, download=True)\n",
        "train_data_loader = DataLoader(train_data, params['batch_size'], shuffle=False)\n",
        "first_batch = train_data_loader.__iter__().__next__()  # first batch of MNIST data set : torch.Size([64x, 1, 28, 28])\n",
        "print(first_batch[0][0].shape)  # torch.Size([1, 28, 28])\n",
        "#imshow(first_batch[0][0])  # plot the image of first batch"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:02, 3632858.25it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 56942.21it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:01, 881793.27it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 21549.84it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n",
            "torch.Size([1, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxREQRNHrbZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input image for defense GAN\n",
        "# For the test purpose, we will use MNIST data sample first.\n",
        "# fgsm_image : torch.Size([1, 28, 28]). This is image x.\n",
        "fgsm_image = first_batch[0][0]  # torch.Size([1, 28, 28]). This should be fgsm_image later on.\n",
        "\n",
        "# evalFunc 에서 numpy 형태로 계산하려고 모양 수정 / 일단은 그냥 28, 28로 했는데 나중엔 일렬로 계산해도 될 듯.\n",
        "x = fgsm_image.view(28,28).numpy()\n",
        "#imshow(fgsm_image)\n",
        "\n",
        "# individual은 numpy array\n",
        "# numpy array 가 들어오면 -> tensor로 바꾸고, netG input 모양에 맞춰줌.\n",
        "# netG의 output이 tensor 형태이므로, numpy로 바꿔서 계산.. (이부분 텐서 형태에서 계산으로 추후 수정하면 될 듯.)\n",
        "def evalFunc(individual):\n",
        "    individual = torch.from_numpy(individual).view(1, 100, 1, 1)\n",
        "    return np.linalg.norm(netG(individual).view(28, 28).detach().numpy() - x, ord=2)**2,\n",
        "\n",
        "# Initial population for GA\n",
        "# initial_population : torch.Size([100, 100, 1, 1]), This has 100 latent vectors z (z is torch.Size([100, 1, 1])).\n",
        "# for example, initial_population[0] is z_0, initial_population[1] is z_1, ..., initial_population[99] is z_99.\n",
        "initial_population = torch.FloatTensor(params['pop_size'], params['nz'], 1, 1).normal_(0, 1)\n",
        "#print(initial_population.shape)  # torch.Size([100, 100, 1, 1])\n",
        "\n",
        "\n",
        "# 이 부분은 evalFunc 이 잘 작동하는지 확인하는 부분입니당.\n",
        "# z 가 길이 100짜리 1차원 numpy array 라고 가정하고(GA 에서 기본 individual 형태) evalFunc에 넣은뒤, 출력\n",
        "# z = torch.FloatTensor(1, params['nz'], 1, 1).normal_(0, 1)\n",
        "# z = z.view(100).numpy()\n",
        "# print(evalFunc(z))\n",
        "\n",
        "# initial_population를 numpy로 타입 맞춰주기.\n",
        "initial_population = initial_population.view(100, 100).numpy()\n",
        "\n",
        "def initIndividual(icls, content):\n",
        "  return icls(content)\n",
        "\n",
        "def initPopulation(pcls, ind_init):\n",
        "  return pcls(ind_init(c) for c in initial_population)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "navWYmQBsJUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from deap import creator, base, tools, algorithms\n",
        "\n",
        "# TOCHECK:  will we use algorithm modules?\n",
        "\n",
        "'''\n",
        "Fitness : single optimization, minimize ||G(z) - x||^2_2\n",
        "'''\n",
        "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
        "creator.create(\"Individual\", np.ndarray, fitness=creator.FitnessMin) # minimizing the fitness value\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "\n",
        "'''\n",
        "[HYPERPARAMETERS]\n",
        "List of Floats: individuals composed of 'IND_SIZE' floating point numbers\n",
        "IND_SIZE: z의 dimension으로 보면 될듯\n",
        "POPULATION\n",
        "CXPB: probability of crossover\n",
        "MUTPB: probability of mutation\n",
        "'''\n",
        "IND_SIZE = 100\n",
        "POPULATION = 100\n",
        "CXPB, MUTPB = 0.2, 0.2\n",
        "GENERATIONS = 100\n",
        "\n",
        "toolbox.register(\"attr_float\", random.random)\n",
        "# toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=IND_SIZE)\n",
        "toolbox.register(\"individual\", initIndividual, creator.Individual)\n",
        "toolbox.register(\"population\", initPopulation, list, toolbox.individual)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO-h_eSNry73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cxTwoPointCopy(ind1, ind2):\n",
        "    \"\"\"Execute a two points crossover with copy on the input individuals. The\n",
        "    copy is required because the slicing in numpy returns a view of the data,\n",
        "    which leads to a self overwritting in the swap operation. It prevents\n",
        "    ::\n",
        "    \n",
        "        >>> import numpy\n",
        "        >>> a = numpy.array((1,2,3,4))\n",
        "        >>> b = numpy.array((5,6,7,8))\n",
        "        >>> a[1:3], b[1:3] = b[1:3], a[1:3]\n",
        "        >>> print(a)\n",
        "        [1 6 7 4]\n",
        "        >>> print(b)\n",
        "        [5 6 7 8]\n",
        "    \"\"\"\n",
        "    size = len(ind1)\n",
        "    cxpoint1 = random.randint(1, size)\n",
        "    cxpoint2 = random.randint(1, size - 1)\n",
        "    if cxpoint2 >= cxpoint1:\n",
        "        cxpoint2 += 1\n",
        "    else: # Swap the two cx points\n",
        "        cxpoint1, cxpoint2 = cxpoint2, cxpoint1\n",
        "\n",
        "    ind1[cxpoint1:cxpoint2], ind2[cxpoint1:cxpoint2] \\\n",
        "        = ind2[cxpoint1:cxpoint2].copy(), ind1[cxpoint1:cxpoint2].copy()\n",
        "        \n",
        "    return ind1, ind2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_HRnfCXruGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Evaluation Function: ||G(z) - x||^2_2 <- use 'np.linalg.norm( , ord=2)**2'\n",
        "'''\n",
        "# TODO : need to change mate, mutate, select operator\n",
        "# can check in here : https://deap.readthedocs.io/en/master/api/tools.html\n",
        "toolbox.register(\"evaluate\", evalFunc)\n",
        "toolbox.register(\"mate\", cxTwoPointCopy)\n",
        "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO_oSiTyr6gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    random.seed(777)\n",
        "\n",
        "    # pop = toolbox.population(n=POPULATION)\n",
        "    pop = toolbox.population()\n",
        "    \n",
        "    print(\"Start of evolution\")\n",
        "\n",
        "    # Evaluate the entire population\n",
        "    # print(fitnesses) -> [(84,), (105,), (96,), (104,), (94,),  ... ] 이런식으로 저장됨.\n",
        "    fitnesses = list(map(toolbox.evaluate, pop))\n",
        "    for ind, fit in zip(pop, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    # Extracting all the fitnesses of \n",
        "    fits = [ind.fitness.values[0] for ind in pop]\n",
        "\n",
        "    # Variable keeping track of the number of generations\n",
        "    g = 0\n",
        "    \n",
        "    # Begin the evolution\n",
        "    while min(fits) > 10 and g < GENERATIONS:\n",
        "        # A new generation\n",
        "        g = g + 1\n",
        "        print(\"-- Generation %i --\" % g)\n",
        "        \n",
        "        # Select the next generation individuals\n",
        "        # len(pop) -> 50, len(pop[0]) -> 5\n",
        "        offspring = toolbox.select(pop, len(pop))\n",
        "\n",
        "        # Clone the selected individuals\n",
        "        offspring = list(map(toolbox.clone, offspring))\n",
        "    \n",
        "        # Apply crossover and mutation on the offspring\n",
        "        '''\n",
        "        they modify those individuals within the toolbox container \n",
        "        and we do not need to reassign their results.\n",
        "        '''\n",
        "        # TODO: want p_new1 = p_m - beta(p_m - p_d), p_new2 = p_m + beta(p_m - p_d)\n",
        "        # want to customize mutation method... there is no proper mutation operator in deap.tools...\n",
        "\n",
        "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
        "            if random.random() < CXPB:\n",
        "                toolbox.mate(child1, child2)\n",
        "                del child1.fitness.values\n",
        "                del child2.fitness.values\n",
        "\n",
        "        for mutant in offspring:\n",
        "            if random.random() < MUTPB:\n",
        "                toolbox.mutate(mutant)\n",
        "                del mutant.fitness.values\n",
        "    \n",
        "        # Evaluate the individuals with an invalid fitness\n",
        "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
        "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
        "        for ind, fit in zip(invalid_ind, fitnesses):\n",
        "            ind.fitness.values = fit\n",
        "        \n",
        "        # The population is entirely replaced by the offspring\n",
        "        pop[:] = offspring\n",
        "\n",
        "        # Gather all the fitnesses in one list and print the stats\n",
        "        fits = [ind.fitness.values[0] for ind in pop]\n",
        "        \n",
        "        length = len(pop)\n",
        "        mean = sum(fits) / length\n",
        "        sum2 = sum(x*x for x in fits)\n",
        "        std = abs(sum2 / length - mean**2)**0.5\n",
        "        \n",
        "        print(\"  Min %s\" % min(fits))\n",
        "        print(\"  Max %s\" % max(fits))\n",
        "        print(\"  Avg %s\" % mean)\n",
        "        print(\"  Std %s\" % std)\n",
        "\n",
        "\n",
        "    print(\"-- End of (successful) evolution --\")\n",
        "    \n",
        "    best_ind = tools.selBest(pop, 1)[0]\n",
        "    print(\"Best individual is %s, %s\" % (best_ind, best_ind.fitness.values))\n",
        "    print(best_ind.fitness.values)\n",
        "    return best_ind\n",
        "# main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kb7nChgmRDK",
        "colab_type": "code",
        "outputId": "7f6b66d7-df36-4e43-8896-d50329fefbe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# For each generation, select the latent vector z* that minimizes fitness, and do the following.\n",
        "# z = torch.FloatTensor(1, params['nz'], 1, 1).normal_(0, 1)  # torch.Size([100, 1, 1]). This should be z* later on.\n",
        "output = np.array(main())\n",
        "\n",
        "z = torch.from_numpy(output).view(1, 100, 1, 1)\n",
        "\n",
        "gen_image = netG(z)  # torch.Size([1, 28, 28]). This is the generated image that we want to see for each generation.\n",
        "# Because gen_image should step closer to fgsm_image x for each generation.\n",
        "print(\"the shape of generated image : \" + str(gen_image.shape))\n",
        "imshow(gen_image.detach())  # plot the image of generated image\n",
        "\n",
        "# After GA, give generated image as input to each classifier (use gen_image)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of evolution\n",
            "-- Generation 1 --\n",
            "  Min 63.412510144468115\n",
            "  Max 215.61453505455574\n",
            "  Avg 132.70238100301552\n",
            "  Std 27.250620733658643\n",
            "-- Generation 2 --\n",
            "  Min 63.412510144468115\n",
            "  Max 169.05182663511732\n",
            "  Avg 111.77037914782228\n",
            "  Std 25.40015849665994\n",
            "-- Generation 3 --\n",
            "  Min 61.780786453602786\n",
            "  Max 184.14623762144765\n",
            "  Avg 93.78120856556599\n",
            "  Std 26.83037261498026\n",
            "-- Generation 4 --\n",
            "  Min 58.44023592464714\n",
            "  Max 113.80883322778482\n",
            "  Avg 73.08886922409526\n",
            "  Std 14.111679140611788\n",
            "-- Generation 5 --\n",
            "  Min 58.44023592464714\n",
            "  Max 135.15783173969248\n",
            "  Avg 66.73162264602772\n",
            "  Std 8.753278057874512\n",
            "-- Generation 6 --\n",
            "  Min 58.44023592464714\n",
            "  Max 104.78549833631041\n",
            "  Avg 64.29723542705324\n",
            "  Std 6.5454431810383165\n",
            "-- Generation 7 --\n",
            "  Min 56.211704575440535\n",
            "  Max 91.77892014669487\n",
            "  Avg 63.33888144159836\n",
            "  Std 6.817360021221939\n",
            "-- Generation 8 --\n",
            "  Min 52.11640306993672\n",
            "  Max 98.16538349731127\n",
            "  Avg 62.03126005800404\n",
            "  Std 6.222405508844501\n",
            "-- Generation 9 --\n",
            "  Min 52.11640306993672\n",
            "  Max 101.10502569959499\n",
            "  Avg 60.18585313669719\n",
            "  Std 7.03936942673624\n",
            "-- Generation 10 --\n",
            "  Min 50.97337464303587\n",
            "  Max 101.54423085054441\n",
            "  Avg 59.789777159216776\n",
            "  Std 8.15533309646316\n",
            "-- Generation 11 --\n",
            "  Min 50.97337464303587\n",
            "  Max 119.0971515202582\n",
            "  Avg 58.6118724663202\n",
            "  Std 8.90254994600995\n",
            "-- Generation 12 --\n",
            "  Min 50.97337464303587\n",
            "  Max 96.40076135558866\n",
            "  Avg 58.25392333399507\n",
            "  Std 9.138501661955914\n",
            "-- Generation 13 --\n",
            "  Min 50.97337464303587\n",
            "  Max 108.4296576101915\n",
            "  Avg 57.85388534760012\n",
            "  Std 10.571144561388406\n",
            "-- Generation 14 --\n",
            "  Min 48.73587154134498\n",
            "  Max 108.24357906713794\n",
            "  Avg 55.546186301995874\n",
            "  Std 9.055155159146505\n",
            "-- Generation 15 --\n",
            "  Min 46.435946052836016\n",
            "  Max 101.73954416292054\n",
            "  Avg 54.75913761062925\n",
            "  Std 10.129023079736154\n",
            "-- Generation 16 --\n",
            "  Min 45.811266981996596\n",
            "  Max 117.32069098678858\n",
            "  Avg 54.59212219946603\n",
            "  Std 10.77965152418813\n",
            "-- Generation 17 --\n",
            "  Min 44.07124251795267\n",
            "  Max 94.35399098848939\n",
            "  Avg 51.76108646584249\n",
            "  Std 8.056747552235262\n",
            "-- Generation 18 --\n",
            "  Min 42.51221541397376\n",
            "  Max 122.53919243553901\n",
            "  Avg 51.74069846404712\n",
            "  Std 10.324594412918175\n",
            "-- Generation 19 --\n",
            "  Min 42.4316609543539\n",
            "  Max 131.35551485049746\n",
            "  Avg 51.33687462267491\n",
            "  Std 12.414811616493571\n",
            "-- Generation 20 --\n",
            "  Min 39.307936900961295\n",
            "  Max 133.17985329864678\n",
            "  Avg 49.87045369928312\n",
            "  Std 13.562225601961517\n",
            "-- Generation 21 --\n",
            "  Min 39.307936900961295\n",
            "  Max 97.46197127417054\n",
            "  Avg 47.051048993323455\n",
            "  Std 9.778329950279147\n",
            "-- Generation 22 --\n",
            "  Min 39.307936900961295\n",
            "  Max 98.12286824570401\n",
            "  Avg 47.706544751293514\n",
            "  Std 12.637319877537259\n",
            "-- Generation 23 --\n",
            "  Min 38.13147536751876\n",
            "  Max 100.64752157606199\n",
            "  Avg 46.92723695331874\n",
            "  Std 13.048985966886752\n",
            "-- Generation 24 --\n",
            "  Min 35.61730211138274\n",
            "  Max 135.60263738802405\n",
            "  Avg 45.83520965013865\n",
            "  Std 14.38222969566732\n",
            "-- Generation 25 --\n",
            "  Min 35.61730211138274\n",
            "  Max 120.06585489423378\n",
            "  Avg 43.710610660527664\n",
            "  Std 10.429173527029592\n",
            "-- Generation 26 --\n",
            "  Min 35.61730211138274\n",
            "  Max 90.25846215056026\n",
            "  Avg 44.068816031309744\n",
            "  Std 11.0789498864029\n",
            "-- Generation 27 --\n",
            "  Min 34.632840441245435\n",
            "  Max 94.42874458763981\n",
            "  Avg 41.48526561411845\n",
            "  Std 8.926535915577844\n",
            "-- Generation 28 --\n",
            "  Min 34.632840441245435\n",
            "  Max 129.99395729601383\n",
            "  Avg 44.92855949109025\n",
            "  Std 16.639543529698635\n",
            "-- Generation 29 --\n",
            "  Min 32.60719354185221\n",
            "  Max 136.02534903750347\n",
            "  Avg 43.467321372453604\n",
            "  Std 17.43742521862794\n",
            "-- Generation 30 --\n",
            "  Min 32.60719354185221\n",
            "  Max 77.92383627247546\n",
            "  Avg 39.67931546976651\n",
            "  Std 7.131365589248071\n",
            "-- Generation 31 --\n",
            "  Min 31.469950789333097\n",
            "  Max 104.28036457536018\n",
            "  Avg 41.11995537540581\n",
            "  Std 12.752243469269152\n",
            "-- Generation 32 --\n",
            "  Min 31.469950789333097\n",
            "  Max 85.96102377132411\n",
            "  Avg 41.61643729180957\n",
            "  Std 12.505492107497545\n",
            "-- Generation 33 --\n",
            "  Min 31.086365211377597\n",
            "  Max 121.87330279014077\n",
            "  Avg 42.132379659076356\n",
            "  Std 17.642012426448026\n",
            "-- Generation 34 --\n",
            "  Min 31.086365211377597\n",
            "  Max 134.34976845799247\n",
            "  Avg 40.939878670352535\n",
            "  Std 16.440579522364388\n",
            "-- Generation 35 --\n",
            "  Min 31.00582957406823\n",
            "  Max 79.71184855400588\n",
            "  Avg 36.65846708605252\n",
            "  Std 8.595623175775776\n",
            "-- Generation 36 --\n",
            "  Min 30.572048219069984\n",
            "  Max 111.2660449891564\n",
            "  Avg 36.46428336590898\n",
            "  Std 12.243316331316125\n",
            "-- Generation 37 --\n",
            "  Min 30.46021951809621\n",
            "  Max 83.41819354638938\n",
            "  Avg 37.12492245410866\n",
            "  Std 12.084483930104184\n",
            "-- Generation 38 --\n",
            "  Min 28.792565962729896\n",
            "  Max 114.33492246782771\n",
            "  Avg 35.222053525985245\n",
            "  Std 10.63356371629071\n",
            "-- Generation 39 --\n",
            "  Min 28.792565962729896\n",
            "  Max 77.50942351123376\n",
            "  Avg 33.679597234290895\n",
            "  Std 6.451097017324752\n",
            "-- Generation 40 --\n",
            "  Min 27.246590760319123\n",
            "  Max 92.3495822605073\n",
            "  Avg 33.46395936293775\n",
            "  Std 8.353495385784907\n",
            "-- Generation 41 --\n",
            "  Min 25.97390316136648\n",
            "  Max 97.61519373635474\n",
            "  Avg 33.70181221840757\n",
            "  Std 9.167240306130648\n",
            "-- Generation 42 --\n",
            "  Min 25.97390316136648\n",
            "  Max 60.68988683037992\n",
            "  Avg 31.439588481653356\n",
            "  Std 5.774294342603829\n",
            "-- Generation 43 --\n",
            "  Min 25.97390316136648\n",
            "  Max 81.44664177042523\n",
            "  Avg 31.699456914748534\n",
            "  Std 7.320444650073669\n",
            "-- Generation 44 --\n",
            "  Min 25.97390316136648\n",
            "  Max 66.61135836222911\n",
            "  Avg 31.44299537230587\n",
            "  Std 6.391673507243843\n",
            "-- Generation 45 --\n",
            "  Min 25.350556024577827\n",
            "  Max 63.966145225386754\n",
            "  Avg 30.874347403997454\n",
            "  Std 5.449145924085703\n",
            "-- Generation 46 --\n",
            "  Min 25.350556024577827\n",
            "  Max 69.49863430942423\n",
            "  Avg 30.693857685584735\n",
            "  Std 6.55314320206509\n",
            "-- Generation 47 --\n",
            "  Min 23.669243501557503\n",
            "  Max 63.03994360160141\n",
            "  Avg 30.043183279459708\n",
            "  Std 5.79570240940168\n",
            "-- Generation 48 --\n",
            "  Min 23.669243501557503\n",
            "  Max 75.42881815621877\n",
            "  Avg 29.24449141943911\n",
            "  Std 6.656231723531428\n",
            "-- Generation 49 --\n",
            "  Min 22.996036312142905\n",
            "  Max 78.27071048745074\n",
            "  Avg 28.57425470003277\n",
            "  Std 6.997767476782045\n",
            "-- Generation 50 --\n",
            "  Min 23.669243501557503\n",
            "  Max 49.61445168019054\n",
            "  Avg 27.33609350206142\n",
            "  Std 4.739461419307418\n",
            "-- Generation 51 --\n",
            "  Min 22.917498167337044\n",
            "  Max 61.54140318477607\n",
            "  Avg 28.296469683702263\n",
            "  Std 7.850381709187893\n",
            "-- Generation 52 --\n",
            "  Min 22.917498167337044\n",
            "  Max 79.0523193419076\n",
            "  Avg 27.74293726810809\n",
            "  Std 8.16824653769014\n",
            "-- Generation 53 --\n",
            "  Min 23.108635023469105\n",
            "  Max 62.79816107038732\n",
            "  Avg 26.91558645130324\n",
            "  Std 6.8441418334561686\n",
            "-- Generation 54 --\n",
            "  Min 23.108635023469105\n",
            "  Max 78.67662727913648\n",
            "  Avg 25.822381793785716\n",
            "  Std 8.135711073295312\n",
            "-- Generation 55 --\n",
            "  Min 23.108635023469105\n",
            "  Max 64.77069316244979\n",
            "  Avg 26.41194898079705\n",
            "  Std 6.320193081960723\n",
            "-- Generation 56 --\n",
            "  Min 22.91878564261083\n",
            "  Max 67.13990521369942\n",
            "  Avg 25.2762127946328\n",
            "  Std 6.130710671531004\n",
            "-- Generation 57 --\n",
            "  Min 22.519581005337614\n",
            "  Max 56.67689187715064\n",
            "  Avg 25.5749631845168\n",
            "  Std 6.18719930114564\n",
            "-- Generation 58 --\n",
            "  Min 22.519581005337614\n",
            "  Max 50.29261577408056\n",
            "  Avg 24.653422092383007\n",
            "  Std 4.183365447407959\n",
            "-- Generation 59 --\n",
            "  Min 22.273982421160326\n",
            "  Max 47.6043818605433\n",
            "  Avg 25.28290136300973\n",
            "  Std 5.3018224166759165\n",
            "-- Generation 60 --\n",
            "  Min 22.273982421160326\n",
            "  Max 158.24478498180179\n",
            "  Avg 26.202416124953906\n",
            "  Std 14.554001010980095\n",
            "-- Generation 61 --\n",
            "  Min 21.4838327044547\n",
            "  Max 71.12952504775967\n",
            "  Avg 26.097149260678954\n",
            "  Std 8.481974910278206\n",
            "-- Generation 62 --\n",
            "  Min 21.36717068699022\n",
            "  Max 98.82886568055619\n",
            "  Avg 25.394912486793054\n",
            "  Std 9.074827523592788\n",
            "-- Generation 63 --\n",
            "  Min 21.321797869750526\n",
            "  Max 96.61283107200234\n",
            "  Avg 26.373821364139168\n",
            "  Std 10.074714354163067\n",
            "-- Generation 64 --\n",
            "  Min 21.321797869750526\n",
            "  Max 62.77607267483313\n",
            "  Avg 24.652650167991133\n",
            "  Std 6.582328644868662\n",
            "-- Generation 65 --\n",
            "  Min 21.321797869750526\n",
            "  Max 79.6543346552653\n",
            "  Avg 25.821600353110306\n",
            "  Std 8.616654060355577\n",
            "-- Generation 66 --\n",
            "  Min 21.217743770377865\n",
            "  Max 47.161237480418094\n",
            "  Avg 23.592651457502352\n",
            "  Std 4.133328126156598\n",
            "-- Generation 67 --\n",
            "  Min 21.217743770377865\n",
            "  Max 48.94286582362133\n",
            "  Avg 23.859568423271295\n",
            "  Std 5.703711593520852\n",
            "-- Generation 68 --\n",
            "  Min 21.217743770377865\n",
            "  Max 64.49526292818427\n",
            "  Avg 24.229412663683206\n",
            "  Std 6.770369856082302\n",
            "-- Generation 69 --\n",
            "  Min 21.217743770377865\n",
            "  Max 108.19572041698939\n",
            "  Avg 24.592116360396723\n",
            "  Std 10.922585458893604\n",
            "-- Generation 70 --\n",
            "  Min 20.950816000415443\n",
            "  Max 60.32011464424522\n",
            "  Avg 24.561759703124952\n",
            "  Std 7.185983588706134\n",
            "-- Generation 71 --\n",
            "  Min 20.950816000415443\n",
            "  Max 77.00848488837073\n",
            "  Avg 24.819087241124063\n",
            "  Std 9.099439077327824\n",
            "-- Generation 72 --\n",
            "  Min 20.950816000415443\n",
            "  Max 67.94203418999041\n",
            "  Avg 23.89934942361976\n",
            "  Std 7.09437049781083\n",
            "-- Generation 73 --\n",
            "  Min 20.950816000415443\n",
            "  Max 95.93419869514855\n",
            "  Avg 24.30445407433652\n",
            "  Std 10.265285212658464\n",
            "-- Generation 74 --\n",
            "  Min 20.950816000415443\n",
            "  Max 56.72025805836256\n",
            "  Avg 23.143653668439402\n",
            "  Std 5.334222476781252\n",
            "-- Generation 75 --\n",
            "  Min 20.950816000415443\n",
            "  Max 67.75733564388156\n",
            "  Avg 23.898991674341723\n",
            "  Std 7.334792025212661\n",
            "-- Generation 76 --\n",
            "  Min 20.950816000415443\n",
            "  Max 40.54134383841665\n",
            "  Avg 23.552631655070737\n",
            "  Std 5.426111501987455\n",
            "-- Generation 77 --\n",
            "  Min 20.734125008159253\n",
            "  Max 53.10055229818522\n",
            "  Avg 23.519747385641185\n",
            "  Std 6.255502220637712\n",
            "-- Generation 78 --\n",
            "  Min 20.734125008159253\n",
            "  Max 141.7641031048006\n",
            "  Avg 26.258874193253067\n",
            "  Std 15.0628781850917\n",
            "-- Generation 79 --\n",
            "  Min 20.734125008159253\n",
            "  Max 71.08235187621267\n",
            "  Avg 23.98184043625114\n",
            "  Std 7.8010122781404725\n",
            "-- Generation 80 --\n",
            "  Min 20.734125008159253\n",
            "  Max 65.8935479954389\n",
            "  Avg 24.64574767527879\n",
            "  Std 9.546599148635494\n",
            "-- Generation 81 --\n",
            "  Min 20.734125008159253\n",
            "  Max 66.9755460279739\n",
            "  Avg 24.612641647693927\n",
            "  Std 8.802663482439025\n",
            "-- Generation 82 --\n",
            "  Min 20.734125008159253\n",
            "  Max 68.58459841298463\n",
            "  Avg 23.705617235113387\n",
            "  Std 7.412081848726056\n",
            "-- Generation 83 --\n",
            "  Min 20.734125008159253\n",
            "  Max 61.52813186897424\n",
            "  Avg 23.660941493391018\n",
            "  Std 6.731492008232349\n",
            "-- Generation 84 --\n",
            "  Min 20.734125008159253\n",
            "  Max 52.894423950440114\n",
            "  Avg 23.83011729305528\n",
            "  Std 7.378651463376029\n",
            "-- Generation 85 --\n",
            "  Min 20.734125008159253\n",
            "  Max 124.68035218532259\n",
            "  Avg 24.30864894545536\n",
            "  Std 12.097250381142377\n",
            "-- Generation 86 --\n",
            "  Min 20.734125008159253\n",
            "  Max 69.3285207676754\n",
            "  Avg 23.379162873695687\n",
            "  Std 7.952366110243676\n",
            "-- Generation 87 --\n",
            "  Min 20.734125008159253\n",
            "  Max 41.251552132947836\n",
            "  Avg 22.290035086898516\n",
            "  Std 4.27889541348232\n",
            "-- Generation 88 --\n",
            "  Min 20.734125008159253\n",
            "  Max 54.07024387957995\n",
            "  Avg 22.507307960808003\n",
            "  Std 5.72852800235694\n",
            "-- Generation 89 --\n",
            "  Min 20.734125008159253\n",
            "  Max 80.5683675426153\n",
            "  Avg 23.510898626445922\n",
            "  Std 8.224309584528845\n",
            "-- Generation 90 --\n",
            "  Min 20.734125008159253\n",
            "  Max 82.39086681603021\n",
            "  Avg 25.971595956882993\n",
            "  Std 11.447814346567277\n",
            "-- Generation 91 --\n",
            "  Min 20.734125008159253\n",
            "  Max 55.59088852885998\n",
            "  Avg 23.1882449389635\n",
            "  Std 6.314237397229266\n",
            "-- Generation 92 --\n",
            "  Min 20.734125008159253\n",
            "  Max 57.198092119579314\n",
            "  Avg 23.379543162300823\n",
            "  Std 6.556978663187971\n",
            "-- Generation 93 --\n",
            "  Min 20.734125008159253\n",
            "  Max 66.37885090039708\n",
            "  Avg 23.27949788011084\n",
            "  Std 7.245160756406828\n",
            "-- Generation 94 --\n",
            "  Min 20.734125008159253\n",
            "  Max 89.36263606268858\n",
            "  Avg 25.6973950803743\n",
            "  Std 11.03689435890327\n",
            "-- Generation 95 --\n",
            "  Min 20.734125008159253\n",
            "  Max 62.88200838323269\n",
            "  Avg 24.82970273007363\n",
            "  Std 7.658153348037465\n",
            "-- Generation 96 --\n",
            "  Min 20.734125008159253\n",
            "  Max 122.93878070829669\n",
            "  Avg 26.634386415641103\n",
            "  Std 15.581649619481576\n",
            "-- Generation 97 --\n",
            "  Min 20.734125008159253\n",
            "  Max 57.22046052769656\n",
            "  Avg 22.85221198327733\n",
            "  Std 6.200992503266248\n",
            "-- Generation 98 --\n",
            "  Min 20.734125008159253\n",
            "  Max 57.596760320277326\n",
            "  Avg 22.89595189349483\n",
            "  Std 6.651107807680438\n",
            "-- Generation 99 --\n",
            "  Min 20.734125008159253\n",
            "  Max 60.2545969955836\n",
            "  Avg 24.626199446752235\n",
            "  Std 8.13913166117582\n",
            "-- Generation 100 --\n",
            "  Min 20.734125008159253\n",
            "  Max 75.78939376144353\n",
            "  Avg 24.03482480468566\n",
            "  Std 8.536443503719877\n",
            "-- End of (successful) evolution --\n",
            "Best individual is [-0.30749586 -0.3380783   0.33576623 -1.3567232  -1.1543083   1.\n",
            " -0.30316368  1.         -0.4514907   1.          0.11854529 -0.7546713\n",
            "  0.76319295  1.          1.4772742   0.          0.          0.\n",
            "  0.          0.          0.0940891  -0.6151224   0.         -0.6095955\n",
            " -0.6319527  -1.0741845   0.          0.10648188 -0.03916005  0.\n",
            "  0.          0.          0.         -1.8204243   0.          0.\n",
            "  0.27141044  0.          0.          0.8072994   0.          1.\n",
            "  0.          0.          0.22237198  0.          0.         -0.01522663\n",
            " -0.23166671  0.          0.5879621  -0.59716344  0.         -1.5764068\n",
            " -0.16220278  1.          0.72485     2.4812963   0.5910441  -0.32965297\n",
            "  1.          0.24269974  0.          1.          0.         -0.9505856\n",
            " -1.0113105   1.5414481  -0.10392367  0.835651    0.         -0.9292008\n",
            " -1.0439506   1.2515764  -1.1781727  -0.74363023  1.962038   -0.6095743\n",
            " -0.4566036  -0.5103421  -0.6048723   0.867079    2.3843617   0.20867401\n",
            "  0.         -1.5689747   0.          2.0964491  -0.06969753  0.67604154\n",
            " -0.15266505  0.         -0.23996991  0.66253924  0.         -0.60131747\n",
            "  1.8732241  -0.37955388 -0.55135113  0.36810395], (20.734125008159253,)\n",
            "(20.734125008159253,)\n",
            "the shape of latent vector : torch.Size([1, 100, 1, 1])\n",
            "the shape of generated image : torch.Size([1, 1, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}